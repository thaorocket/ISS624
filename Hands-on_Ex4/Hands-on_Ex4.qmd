---
title: "Hands-on Exercise 4: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method"
editor: visual
---

## Overview

In this hands-on exercise, we will learn how to build hedonic pricing models by using Geographically weighted regression (GWR) methods. GWR is

a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable).

In our example, the dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.

## Getting Started

The below code chunks install and load the following packages in R environment:

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)
```

R package for building OLS and performing diagnostics tests/test assumption:

-   oslrr

R package for calibrating geographical weighted family of models

-   GW model

R package for multivariate data visualisation and analysis

-   corrplot

Spatial data handling

-   sf

Attribute data handling

-   tidyverse, especially readr, ggplot2, and dplyr

Choropleth mapping

-   tmap

## Geospatial Data Wrangling

### Importing geospatial data

The code chunks below import geospatial data into R environment. The data is in ESRI shapefile and consists of URA Master Plan 2014's planning subzone boundaries. GIS data is in svy21 projected coordinates system.

```{r}
mpsz<-st_read(dsn="data/geospatial",layer="MP14_SUBZONE_WEB_PL")
```

The above summary shows that the R object is called mpsz and is in sf format with 323 rows and 15 columns. The geometry type is multipolygon. Projected CRS is svy21 and there is no information on EPSG.

### Updating CRS information

The code chunk below updates the newly import mpsz object with the correct EPSG code of 3414.

```{r}
mpsz_svy21<-st_transform(mpsz,crs=3414)
```

After transforming the coordinates system of the data, we can use st_crs() function of sf package to verify the projection system:

```{r}
st_crs(mpsz_svy21)
```

Notice that the EPSG code is now 3414.

Now, in order to view the extent of mpsz_svy21, we use st_bbox() of sf package:

```{r}
st_bbox(mpsz_svy21)
```

## Aspatial Data Wrangling

### Importing the aspatial data

The code chunks below is used to import csv file into R environment as tibble data frame . In this example, the file we use is called the Condo_resale_2015 and the new data frame will be called condo_resale.

```{r}
condo_resale<-read_csv("data/aspatial/Condo_resale_2015.csv")
```

After importing the data into R, we can use glimpse() to display the data structure.

```{r}
glimpse(condo_resale)
```

We also can check on the coordinates, longitude and latitude respectively:

```{r}
head(condo_resale$LONGITUDE)
```

```{r}
head(condo_resale$LATITUDE)
```

Next, we can use summary() function to display the summary statistics of condo_resale tibble data frame:

```{r}
summary(condo_resale)
```

Converting aspatial data frame into an sf object

Currently the condo_resale object is in tibble data frame object and we would like to convert it into sf data frame. We can do so by using st_as_sf() of sf package.

```{r}
condo_resale.sf<-st_as_sf(condo_resale,coords = c("LONGITUDE","LATITUDE"),crs=4326)%>%
  st_transform(crs=3414)
```

Notice that st_transform() in the above code chunks is used to transform from wgs84 (crs is 4326) to svy21 (crs is 3414).

Next, we can list the content of condo_resale.sf object.

```{r}
head(condo_resale.sf)
```

## Exploratory Data Analysis (EDA)

### EDA using statistical graphics

We can plot the distribution of SELLING_PRICE by using appropriate EDA tool shown below. In this case we will use histogram.

```{r}
ggplot(data=condo_resale.sf,aes(x=`SELLING_PRICE`))+
  geom_histogram(bins=20,color="black",fill="light blue")
```

The above figure shows a right skewed distribution, which means that more condo units were transacted at lower prices.

Statistically, the skewed distribution can be normalized by using log transformation. The below code chunks derive a new variable called LOG_SELLING_PRICE by using log transformation on the SELLING_PRICE variable.

```{r}
condo_resale.sf<-condo_resale.sf%>%
  mutate(`LOG_SELLING_PRICE`=log(`SELLING_PRICE`))
```

Now, we can plot the new variable LOG_SELLING_PRICE using the code chunks below:

```{r}
ggplot(data=condo_resale.sf,aes(x=`LOG_SELLING_PRICE`))+
  geom_histogram(bins=20,color="black",fill="light blue")
```

Notice that the distribution has become less skewed after the transformation.

### Multiple Histogram Plots distribution of variables

In this section, we will plot multiple histograms (known as trellis plot) by using ggarrange() of ggpubr package.

The below code chunks create 12 histograms based on various attributes of condo_resale.sf data frame. Then ggarrange() is used to organize these histograms into a 3 columns x 4 rows small multiple plot.

```{r}
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

### Drawing Statistical Point Map

Lastly, we use tmap package to reveal the geospatial distribution of condo resale prices in Singapore.

First, we will turn on the interactive mode of tmap by using the below code chunks.

```{r}
tmap_mode("view")
```

Next, the code chunks below create an interactive point symbol map.

```{r}
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz_svy21)+
  tm_polygons() +
tm_shape(condo_resale.sf)+
  tm_dots(col="SELLING_PRICE",alpha=0.6,style="quantile")+
  tm_view(set.zoom.limits = c(11,14))
```

Some may face the issue of broken polygons (Shape contains invalid polygons). We need to set tmap_options with check.and.fix argument to TRUE.

The point data won't have topology issue. The issue is with polygons or lines. Therefore, we will insert the tmap_options() function after we read the polygons data in.

Compare tm_dots() with tm_bubbles()

In the below code chunks, we use tm_bubbles() to see the difference between tm_dots() and tm_bubbles()

```{r}

```

```{r}
tm_shape(mpsz_svy21)+
  tm_polygons() +
tm_shape(condo_resale.sf)+
  tm_bubbles(col="SELLING_PRICE",alpha=0.6,style="quantile")+
  tm_view(set.zoom.limits = c(11,14))
```

## Hedonic Pricing Modeling in R

In this section, we learn how to build hedonic pricing models for condo resale units using lm() of R base.

### Simple Linear Regression Method

First, we build a simple linear regression model by using SELLING_PRICE as dependent variable and AREA_SQM as independent variable.

```{r}
condo.slr<-lm(formula=SELLING_PRICE~AREA_SQM,data=condo_resale.sf)
```

lm() returns an object of class "lm" for single variable or of class c("mlm","lm") for multiple variables.

summary() and anova() function of baseR can be used to obtain and print a summary and analysis of variance table of results.

```{r}
summary(condo.slr)
```

With the above output, we can come up with a formula for this simple linear model:

\*y=-258121.1+14719.0x1\*

The R-squared of 0.4518 explains that 45.18% of variations in SELLING_PRICE can be explained by variations in AREA_SQM.

Also, since p-value is smaller than 0.0001, we will reject the null hypothesis and infer that the above simple linear regression model is a good estimator of SELLING_PRICE.

To visualize the best fit curve, we can incorporate lm() as a method function in ggplot's geometry as shown in the code chunks below:

```{r}
ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)
```

From the above figure, we notice that there were some outliers with relatively high selling price.

### Multiple Linear Regression Method

#### Visualizing the relationships of the independent variables

Before building multiple linear regression model, it is necessary for us to check for multicollinearity to ensure that the quality of the model will not be compromised by not having independent variables correlated to each other.

The below code chunks use corrplot package to plot a scatterplot matrix of relationship between the independent variables in the condo_resale data frame.

Do take note that we need to use the condo_resale data frame instead of condo_resale.sf data frame because this function does not need geometry data. Condo_resale.sf has geometry info. Therefore, it will prompt error.

```{r}
corrplot(cor(condo_resale[,5:23]),diag=FALSE,order="AOE",tl.pos="td",tl.cex=0.5,method="number",type="upper")
```

Notice that in the above code chunks, AOE was used under order argument. It orders the variables by using the angular order of eigenvectors method.

From the above scatterplot matrix, we notice that FREEHOLD is highly correlated to LEASEHOLD_99YR. Therefore, we will need to exclude either one of the variables in our subsequent model building. In this case, we remove the LEASEHOLD_99YR.

### Building a hedonic pricing model using multiple linear regression method

The code chunks below derives a multiple linear regression model. For lm model, it is okay to use sf data frame.

```{r}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                data=condo_resale.sf)
summary(condo.mlr)
```

### Preparing Publication Quality Table: olsrr method

From the above table summary, we can see that not all independent variables are statistically significant such as PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_TOP_PRIMARY_SCH, PROX_SUPERMARKET, etc. We will then need to revise the model by excluding those variables which are not statistically significant.

The below code chunks derives a revised multiple linear regression model:

```{r}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + 
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale.sf)
ols_regress(condo.mlr1)
```

From the above summary, with adjusted R-square of 0.647, we are able to account for 64.7% of the variations in the dependent variable.

### Preparing Publication Quality Table: gtsummary method

The gtsummary package provides an elegant and flexible way to create publication quality summary tables in R.

In the code chunks below, tbl_regression() is used to create a well-formatted regression report.

```{r}
tbl_regression(condo.mlr1,intercept=TRUE)
```

With the gt summary package, model statistics can be included in the summary table by either appending them to the report table by using add_glance_table() or adding them as a table source note by using add_glance_source_note() as shown in the below code chunks:

```{r}
tbl_regression(condo.mlr1, 
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

Notice that olsrr provide a very tidy table summary with Adjust R-square, etc.

From the p-value we can look at the independent variables and compare with significance level of 0.05, we will know which variable to be removed.

The beta columns give us details such as for 1 unit increase in AREA_SQM, the selling price will increase by 12777.523 or for 1 unit decrease in age, the selling price will decrease by 24687.739.

At the same time, we also notice some interesting observations. For one unit increase in distance to MRT, the price will decrease by 2947745.107.

#### Test for multicollinearity

In the code chunks below, the ols_vif_tol() function of olsrr package is used to test for any signs of multicollinearity on the independent variables.

```{r}
ols_vif_tol(condo.mlr1)
```

#### Test for Non-Linearity

In the code chunks below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.

```{r}
ols_plot_resid_fit(condo.mlr1)
```

The above figure shows that most of the data points are scattered around 0 line. Therefore, we can conclude that the relationships between the dependent variable and independent variables are linear.

#### Test for Normality Assumption

Lastly, the code chunks below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.

```{r}
ols_plot_resid_hist(condo.mlr1)
```

The above graph shows that the distribution of residuals of the multiple linear regression model resembles the normal distribution.

However, in order to conclude if the residuals are normally distributed, we need to perform formal statistics tests. In order to do so, we can use this function ols_test_normality() of olsrr package shown in the below code chunks:

```{r}
ols_test_normality(condo.mlr1)
```

The summary table above shows that the p-values across all four tests are smaller than alpha value of 0.05. Hence, we will reject the null hypothesis and infer that there is sufficient evidence to conclude that the residuals are not normally distributed.

#### Test for Spatial Autocorrelation

First, we export the residual of the hedonic pricing model and save it as a data frame.

```{r}
mlr.output<-as.data.frame(condo.mlr1$residuals)
```

For the above code chunks, if we can check the contents of condo.mlr1, we can see that residuals is one of the field. If we add "\$", we can specifically choose this column. Notice that the newly created mlr.output data frame is in tibble data frame.

Next, we will join the newly created mlr.output data frame with the condo_resale.sf object. Notice that we change the column name from condo.mlr1.residuals to MLR_RES.

```{r}
condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
rename(`MLR_RES` = `condo.mlr1.residuals`)
```

Next, we will need to convert the condo_resale.res.sf from simple feature data frame into SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.

The code chunks below will be used to perform the data conversion process.

```{r}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

Next, we will use tmap package to display the distribution of residuals on an interactive map.

```{r}
tmap_mode("view")
```

The code chunks below creates an interactive point symbol map.

```{r}
tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

We then need to switch back to "plot" mode before continuing.

```{r}
tmap_mode("plot")
```

The above map shows there is some sign of spatial autocorrelation.

We will then use Moran's I test to prove if our observation is true.

First, we will compute the distance-based weight matrix by using dnearneigh() function of spdep package.

```{r}
nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
```

Next, nb2listw() of spdep package will be used to convert the output neighbors lists (i.e. nb) into spatial weights.

```{r}
nb_lw<-nb2listw(nb,style="W")
summary(nb_lw)
```

Next, lm.morantest() of spdep package will be used to perform Moran's I test for residual spatial autocorrelation.

```{r}
lm.morantest(condo.mlr1,nb_lw)
```

The Global Moran's I test for residual spatial autocorrelation shows that its p-value is much less than the alpha value of 0.05. Therefore, we will reject the null hypothesis that the residuals are randomly distributed.

Since the Observed Global Moran's I is 0.1438876 which is greater than 0, we can infer that the residuals resemble cluster distribution.

## Building Hedonic Pricing Models using GW Model

### Building Fixed Bandwidth GWR Model

#### Computing fixed bandwidth

```{r}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                     FAMILY_FRIENDLY + FREEHOLD, 
                   data=condo_resale.sp, 
                   approach="CV", 
                   kernel="gaussian", 
                   adaptive=FALSE, 
                   longlat=FALSE)
```

The function will run multiple times until it stops at a value when there is no change/minimum change. The result shows that the recommended bandwidth is 971.3405 meter. The reason why it was in meter was due to projected coordinates system svy21 (which is in meter) that we use in this exercise.

#### GWModel method-fixed bandwidth

```{r}
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                         FAMILY_FRIENDLY + FREEHOLD, 
                       data=condo_resale.sp, 
                       bw=bw.fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
```

The output is saved under gwrm class. The code below can be used to display the model output.

```{r}
gwr.fixed
```

The reported AICc under GWR model is 42263.61 is higher than the reported AIC calculated under global multiple linear regression model. The reported R-square is 0.8430417, which is significantly better than the reported R-square calculated using the global multiple linear regression model of 0.6472.

### Building Adaptive Bandwidth GWR model

#### Computing the adaptive bandwidth

The code chunks also use bw.gwr() function to compute the bandwidth. However, since we want to compute adaptive bandwidth, the adaptive argument has been changed to TRUE.

```{r}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + 
                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + 
                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + 
                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                      data=condo_resale.sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
```

This method will advise how many data points that we should use. The result shows that 30 is the recommended data points to be used.

#### Constructing the adaptive bandwidth GWR model

In the below code chunks, we calibrate the GWR-based hedonic pricing model by using adaptive bandwidth and gaussian kernel.

```{r}
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + 
                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                          data=condo_resale.sp, bw=bw.adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)
```

We can display the model output by using the below code:

```{r}
gwr.adaptive
```

We notice that AICc of the GWR model of 42263.61 which is significantly smaller than the AIC calculated under the global multiple linear regression model of 42967.14. At the same time, under GWR, adjusted R-square is 0.8430417, which means that this model can explain 84.3% of the times variations in the dependent variables compared to adjusted R-square of 0.6472 under global multiple linear regression model. GWR method proves to be more superior than the global linear regression method in this example.

#### Converting SDF info sf data frame

To visualize the fields in SDF, we need to first convert it into sf data frame by using the below code chunks:

```{r}
condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)
```

```{r}
condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)
condo_resale.sf.adaptive.svy21  
```

```{r}
gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))
```

Next, glimpse() is used to display the content of condo_resale.sf.adaptive sf data frame.

```{r}
glimpse(condo_resale.sf.adaptive)
```

```{r}
summary(gwr.adaptive$SDF$yhat)
```

#### Visualizing local R2

The code chunks below is used to create an interactive point symbol map.

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

```

We then reset tmap_mode back to plot.

```{r}
tmap_mode("plot")
```

#### Visualizing Coefficients Estimates

The code chunks below is used to create an interactive point symbol map.

```{r}
tmap_mode("view")
AREA_SQM_SE <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

AREA_SQM_TV <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_TV",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
             asp=1, ncol=2,
             sync = TRUE)
```

#### By URA Planning Region

```{r}
tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="CENTRAL REGION", ])+
  tm_polygons()+
tm_shape(condo_resale.sf.adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)
```
